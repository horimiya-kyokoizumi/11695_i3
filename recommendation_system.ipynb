{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e9f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76691bcf",
   "metadata": {},
   "source": [
    "## Processing of Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a0e46ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the data into the three different records\n",
    "# 1. <time>,<userid>,recommendation request <server>, status <200 for success>, result: <recommendations>, <responsetime>\n",
    "# 2. <time>,<userid>,GET /data/m/<movieid>/<minute>.mpg\n",
    "# 3. <time>,<userid>,GET /rate/<movieid>=<rating>\n",
    "\n",
    "def sort_records(records):\n",
    "    \"\"\"\n",
    "    Sorts records into the three different entry types\n",
    "    --------------------------------------------------\n",
    "    Args\n",
    "    -----\n",
    "    1. records: list\n",
    "        records obtained from kafka\n",
    "            \n",
    "    Returns\n",
    "    --------\n",
    "    1. recommendation_records: list\n",
    "        records of movie recommendations to users\n",
    "    2. watching_records: list\n",
    "        records of what users watched\n",
    "    3. rating records: list\n",
    "        records of user movie ratings\n",
    "    \"\"\"\n",
    "    recommendation_records = []\n",
    "    watching_records = []\n",
    "    rating_records = []\n",
    "    for record in records:\n",
    "        if \"recommendation\" in record:\n",
    "            recommendation_records.append(record)\n",
    "        elif record[-4:] == \".mpg\":\n",
    "            watching_records.append(record)\n",
    "        elif \"rate\" in record:\n",
    "            rating_records.append(record)\n",
    "        else:\n",
    "            print(f\"Unknown record: {record}\")\n",
    "            \n",
    "    return recommendation_records, watching_records, rating_records\n",
    "\n",
    "data_files = ['data.txt']\n",
    "records = []\n",
    "for data_file in data_files:\n",
    "    with open(data_file) as f:\n",
    "        lines = f.readlines()\n",
    "    records.extend([line[2:-2] for line in lines])\n",
    "\n",
    "recommendation_records, watching_records, rating_records = sort_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6519b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_to_unix(timestamp):\n",
    "    \"\"\"\n",
    "    Converts normal timestamp to unix time\n",
    "    ----------------------------------------\n",
    "    Args\n",
    "    -----\n",
    "    1. timestamp: str\n",
    "        format - YYYY-MM-DDTHH:MM:SS (SS is optional)\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    1. unix_timestamp: float\n",
    "        The equivalent unix epoch time\n",
    "    \"\"\"\n",
    "    date, day_time = timestamp.split(\"T\")\n",
    "    year, month, day = date.split(\"-\")\n",
    "    times = day_time.split(\":\")\n",
    "    hour, minute = times[0], times[1]\n",
    "\n",
    "    date_time_obj = datetime.datetime(int(year), int(month),\n",
    "                                      int(day), int(hour),\n",
    "                                      int(minute)\n",
    "                                      )\n",
    "    unix_timestamp = time.mktime(date_time_obj.timetuple())\n",
    "    return unix_timestamp\n",
    "\n",
    "def rating_records_to_df(rating_records):\n",
    "    \"\"\"\n",
    "    Converts the rating records into a dataframe\n",
    "    Also converts timestamp to unix time\n",
    "    ---------------------------------------------\n",
    "    Args\n",
    "    -----\n",
    "    1. rating_records: list\n",
    "        records of user movie ratings\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    1. rating_records_df: DataFrame\n",
    "        cols - userId, movieId, rating, timestamp\n",
    "    \"\"\"\n",
    "    ratings_dict = {\n",
    "        'userId': [],\n",
    "        'movieId': [],\n",
    "        'rating': [],\n",
    "        'timestamp': []\n",
    "    }\n",
    "    \n",
    "    for record in rating_records:\n",
    "        timestamp, userId, get_request = record.split(\",\")\n",
    "        movieId_rating = get_request.split(\"/\")[-1]\n",
    "        movieId, rating = movieId_rating.split(\"=\")\n",
    "        unix_timestamp = datetime_to_unix(timestamp)\n",
    "        \n",
    "        ratings_dict['userId'].append(userId)\n",
    "        ratings_dict['movieId'].append(movieId)\n",
    "        ratings_dict['rating'].append(rating)\n",
    "        ratings_dict['timestamp'].append(unix_timestamp)\n",
    "        \n",
    "    ratings_df = pd.DataFrame.from_dict(ratings_dict)\n",
    "    return ratings_df\n",
    "\n",
    "def encode_movieIds(ratings_df):\n",
    "    \"\"\"\n",
    "    Converts movie names to a numerical Id\n",
    "    ---------------------------------------\n",
    "    Args\n",
    "    -----\n",
    "    1. ratings_df: DataFrame\n",
    "        records of user movie ratings (with movie names)\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    1. ratings_df: DataFrame\n",
    "        records of user movie ratings (with numerical movie Ids)\n",
    "    2. numerical_movie_id_encoding: dict\n",
    "        maps movie name to a numerical id\n",
    "    \"\"\"\n",
    "    unique_movies = list(set(ratings_df.loc[:, 'movieId']))\n",
    "    numerical_movie_id_encoding = {unique_movies[i]: i for i in range(len(unique_movies))}\n",
    "    ratings_df['movieId'] = ratings_df['movieId'].apply(lambda x: numerical_movie_id_encoding[x])\n",
    "    \n",
    "    return ratings_df, numerical_movie_id_encoding\n",
    "\n",
    "ratings_df = rating_records_to_df(rating_records)\n",
    "ratings_df, numerical_movie_id_encoding = encode_movieIds(ratings_df)\n",
    "ratings_df = ratings_df.astype('int64')\n",
    "data = ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34051df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie_name_to_id.json', 'w') as f:\n",
    "    json.dump(numerical_movie_id_encoding, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c948c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(data, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits the data of each user 80-20 (Not the entire set 80-20)\n",
    "    The split is done such that the test set are the 20% most recent entries\n",
    "    -------------------------------------------------------------------------\n",
    "    Args\n",
    "    -----\n",
    "    1. data: DataFrame\n",
    "        Contains user movie ratings\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    1. train: DataFrame\n",
    "        Train set of user movie ratings\n",
    "    2. test: DataFrame\n",
    "        Test set of user movie ratings\n",
    "    Note: Both share the same format as data\n",
    "    \"\"\"\n",
    "    data['userId'] = data['userId'].astype('str')\n",
    "    data['movieId'] = data['movieId'].astype('str')\n",
    "    users = data['userId'].unique() # list of all users\n",
    "    movies = data['movieId'].unique() # list of all movies\n",
    "    test = pd.DataFrame(columns=data.columns)\n",
    "    train = pd.DataFrame(columns=data.columns)\n",
    "    \n",
    "    for u in users:\n",
    "        temp = data[data['userId'] == u]\n",
    "        n = len(temp)\n",
    "        test_size = int(test_ratio*n)\n",
    "\n",
    "        temp = temp.sort_values('timestamp').reset_index()\n",
    "        temp.drop('index', axis=1, inplace=True)\n",
    "            \n",
    "        # If there is only one entry for the user, it goes to test\n",
    "        dummy_test = temp.loc[n-1-test_size:]\n",
    "        dummy_train = temp.loc[:n-2-test_size]\n",
    "\n",
    "        test = pd.concat([test, dummy_test])\n",
    "        train = pd.concat([train, dummy_train])\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "def check_correct_sorting(data, train, test):\n",
    "    \"\"\"\n",
    "    Confirms that the data was split properly (more recent entries in test)\n",
    "    ------------------------------------------------------------------------\n",
    "    Args\n",
    "    -----\n",
    "    1. data: DataFrame\n",
    "        Full dataframe of user movie ratings (train + test)\n",
    "    2. train: DataFrame\n",
    "        Train set\n",
    "    3. test: DataFrame\n",
    "        Test set\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    1. is_sorted_correctly: boolean\n",
    "        Indicates whether all users were split correctly\n",
    "    \"\"\"\n",
    "    users = set(data['userId'])\n",
    "    is_sorted_correctly = True\n",
    "    for user in users:\n",
    "        num_entries = data[data['userId'] == user].shape[0]\n",
    "        if num_entries < 2:\n",
    "            continue\n",
    "\n",
    "        latest_train_timestamp = train[train['userId'] == user].iloc[-1]['timestamp']\n",
    "        earliest_test_timestamp = test[test['userId'] == user].iloc[0]['timestamp']\n",
    "        if latest_train_timestamp > earliest_test_timestamp:\n",
    "            is_sorted_correctly = False\n",
    "            break\n",
    "\n",
    "    return is_sorted_correctly\n",
    "\n",
    "train, test = create_train_test_split(data)\n",
    "assert True == check_correct_sorting(data, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6583413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85e88cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.astype('int64')\n",
    "test = test.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cff32178",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "865c8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train_df.csv\", index=False)\n",
    "test.to_csv(\"test_df.csv\", index=False)\n",
    "data.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03a2c164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>268254</td>\n",
       "      <td>211</td>\n",
       "      <td>4</td>\n",
       "      <td>1641723840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>740774</td>\n",
       "      <td>238</td>\n",
       "      <td>3</td>\n",
       "      <td>1641724320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>834892</td>\n",
       "      <td>1084</td>\n",
       "      <td>2</td>\n",
       "      <td>1641724560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>337247</td>\n",
       "      <td>1043</td>\n",
       "      <td>4</td>\n",
       "      <td>1643005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>179302</td>\n",
       "      <td>333</td>\n",
       "      <td>1</td>\n",
       "      <td>1643005560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0  268254      211       4  1641723840\n",
       "1  740774      238       3  1641724320\n",
       "2  834892     1084       2  1641724560\n",
       "3  337247     1043       4  1643005200\n",
       "4  179302      333       1  1643005560"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6753efb",
   "metadata": {},
   "source": [
    "## Setup PyTorch Lightning Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf12edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import DeviceStatsMonitor\n",
    "from torch.utils.data import random_split, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da8847dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationDataSet(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.data_indices = list(df.index)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.df.iloc[idx]\n",
    "        x = torch.LongTensor([entry['userId'], entry['movieId']])\n",
    "        y = torch.Tensor([entry['rating']])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d49c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(pl.LightningModule):\n",
    "    def __init__(self, num_users, num_items, train_df, val_df, emb_size=100):\n",
    "        super(MF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        # initializing our matrices with a positive number generally will yield better results\n",
    "        self.user_emb.weight.data.uniform_(0, 0.5)\n",
    "        self.item_emb.weight.data.uniform_(0, 0.5)\n",
    "        \n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        u = self.user_emb(u)\n",
    "        v = self.item_emb(v)\n",
    "        return (u*v).sum(1)  # taking the dot product\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=0.0)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        usernames = x[:, 0]\n",
    "        movies = x[:, 1]\n",
    "        ratings = y[:, 0]\n",
    "        y_hat = self(usernames, movies)\n",
    "        loss = F.mse_loss(y_hat, ratings)\n",
    "        \n",
    "        mse = torchmetrics.MeanSquaredError(ratings, y_hat)\n",
    "        \n",
    "        # Each training step is one epoch\n",
    "        self.logger.experiment.add_scalar(\"mse_loss/train\", loss, self.current_epoch)\n",
    "        \n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        results = self.training_step(batch, batch_idx)\n",
    "        return results\n",
    "    \n",
    "    def validation_epoch_end(self, val_outputs):\n",
    "        val_loss = val_outputs[0]['loss']\n",
    "        \n",
    "        progress_bar = {'val_loss': val_loss}\n",
    "        \n",
    "        self.logger.experiment.add_scalar(\"mse_loss/val\", val_loss, self.current_epoch)\n",
    "        \n",
    "        return {'val_loss': val_loss, 'progress_bar': progress_bar}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_data = RecommendationDataSet(self.train_df)\n",
    "        train_loader = DataLoader(train_data, batch_size=len(train_data))\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_data = RecommendationDataSet(self.val_df)\n",
    "        val_loader = DataLoader(val_data, batch_size=len(val_data))\n",
    "        return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a50ac241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num users:  999627\n",
      "num items:  1882\n"
     ]
    }
   ],
   "source": [
    "print(\"num users: \", max(data['userId'].unique()))\n",
    "print(\"num items: \", max(data['movieId'].unique()))\n",
    "num_users = max(data['userId'].unique())\n",
    "num_items = max(data['movieId'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42feaa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MF(num_users+1, num_items+1, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7f91e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger('lightning_logs', name='lr_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af55bd79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Missing logger folder: lightning_logs\\device_stats\n",
      "\n",
      "  | Name           | Type             | Params \n",
      "-----------------------------------------------------\n",
      "0 | user_emb       | Embedding        | 100.0 M\n",
      "1 | item_emb       | Embedding        | 188 K  \n",
      "2 | mse_calculator | MeanSquaredError | 0      \n",
      "-----------------------------------------------------\n",
      "100 M     Trainable params\n",
      "0         Non-trainable params\n",
      "100 M     Total params\n",
      "400.604   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PREDATOR\\anaconda3\\envs\\i3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\PREDATOR\\anaconda3\\envs\\i3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721f807fddd44c28a635786fe80eec39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FIT Profiler Report\r\n",
      "\r\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "Total                              \t|  -              \t|_              \t|  53.672         \t|  100 %          \t|\r\n",
      "----------------------------------------------------------------------------------------------------------------------------------------\r\n",
      "run_training_epoch                 \t|  5.339          \t|10             \t|  53.39          \t|  99.475         \t|\r\n",
      "on_train_epoch_end                 \t|  3.7841         \t|10             \t|  37.841         \t|  70.504         \t|\r\n",
      "optimizer_step_with_closure_0      \t|  0.7328         \t|10             \t|  7.328          \t|  13.653         \t|\r\n",
      "run_training_batch                 \t|  0.7328         \t|10             \t|  7.328          \t|  13.653         \t|\r\n",
      "fetch_next_train_batch             \t|  0.2071         \t|20             \t|  4.142          \t|  7.7172         \t|\r\n",
      "get_train_batch                    \t|  0.2071         \t|20             \t|  4.142          \t|  7.7172         \t|\r\n",
      "fetch_next_validate_batch          \t|  0.1563         \t|20             \t|  3.126          \t|  5.8243         \t|\r\n",
      "get_validate_batch                 \t|  0.1563         \t|20             \t|  3.126          \t|  5.8243         \t|\r\n",
      "training_step_and_backward         \t|  0.142          \t|10             \t|  1.42           \t|  2.6457         \t|\r\n",
      "backward                           \t|  0.1125         \t|10             \t|  1.125          \t|  2.0961         \t|\r\n",
      "on_validation_start                \t|  0.041364       \t|11             \t|  0.455          \t|  0.84774        \t|\r\n",
      "zero_grad                          \t|  0.0279         \t|10             \t|  0.279          \t|  0.51982        \t|\r\n",
      "fetch_next_sanity_check_batch      \t|  0.078          \t|2              \t|  0.156          \t|  0.29065        \t|\r\n",
      "get_sanity_check_batch             \t|  0.078          \t|2              \t|  0.156          \t|  0.29065        \t|\r\n",
      "validation_step                    \t|  0.0042727      \t|11             \t|  0.047          \t|  0.087569       \t|\r\n",
      "evaluation_step_and_end            \t|  0.0042727      \t|11             \t|  0.047          \t|  0.087569       \t|\r\n",
      "on_train_start                     \t|  0.047          \t|1              \t|  0.047          \t|  0.087569       \t|\r\n",
      "on_validation_end                  \t|  0.0029091      \t|11             \t|  0.032          \t|  0.059621       \t|\r\n",
      "on_sanity_check_start              \t|  0.031          \t|1              \t|  0.031          \t|  0.057758       \t|\r\n",
      "on_train_batch_end                 \t|  0.0031         \t|10             \t|  0.031          \t|  0.057758       \t|\r\n",
      "on_train_end                       \t|  0.016          \t|1              \t|  0.016          \t|  0.029811       \t|\r\n",
      "evaluation_batch_to_device         \t|  0.0014545      \t|11             \t|  0.016          \t|  0.029811       \t|\r\n",
      "training_step                      \t|  0.0016         \t|10             \t|  0.016          \t|  0.029811       \t|\r\n",
      "model_forward                      \t|  0.0016         \t|10             \t|  0.016          \t|  0.029811       \t|\r\n",
      "on_train_epoch_start               \t|  0.0015         \t|10             \t|  0.015          \t|  0.027948       \t|\r\n",
      "configure_callbacks                \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "prepare_data                       \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "on_before_accelerator_backend_setup\t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "setup                              \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "configure_sharded_model            \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "on_configure_sharded_model         \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "configure_optimizers               \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "on_fit_start                       \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "on_pretrain_routine_start          \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "on_pretrain_routine_end            \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "on_val_dataloader                  \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "val_dataloader                     \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "on_validation_model_eval           \t|  0.0            \t|11             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_epoch_start                     \t|  0.0            \t|21             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_validation_epoch_start          \t|  0.0            \t|11             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_validation_batch_start          \t|  0.0            \t|11             \t|  0.0            \t|  0.0            \t|\r\n",
      "validation_step_end                \t|  0.0            \t|11             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_validation_batch_end            \t|  0.0            \t|11             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_validation_epoch_end            \t|  0.0            \t|11             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_epoch_end                       \t|  0.0            \t|21             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_sanity_check_end                \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "on_train_dataloader                \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "train_dataloader                   \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "training_batch_to_device           \t|  0.0            \t|10             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_batch_start                     \t|  0.0            \t|10             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_train_batch_start               \t|  0.0            \t|10             \t|  0.0            \t|  0.0            \t|\r\n",
      "training_step_end                  \t|  0.0            \t|10             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_before_zero_grad                \t|  0.0            \t|10             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_before_backward                 \t|  0.0            \t|10             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_after_backward                  \t|  0.0            \t|10             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_before_optimizer_step           \t|  0.0            \t|10             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_batch_end                       \t|  0.0            \t|10             \t|  0.0            \t|  0.0            \t|\r\n",
      "on_fit_end                         \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "teardown                           \t|  0.0            \t|1              \t|  0.0            \t|  0.0            \t|\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10, log_every_n_steps=1, logger=logger, profiler=\"simple\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a5fbf",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534c63f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: The process \"tensorboard.exe\" not found.\n"
     ]
    }
   ],
   "source": [
    "!Taskkill /IM \"tensorboard.exe\" /F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0af87019",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11788), started 0:05:04 ago. (Use '!kill 11788' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1f07aaae916756dc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1f07aaae916756dc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779fa3df",
   "metadata": {},
   "source": [
    "## Pre PyTorch Lightning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=100):\n",
    "        super(MF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        # initializing our matrices with a positive number generally will yield better results\n",
    "        self.user_emb.weight.data.uniform_(0, 0.5)\n",
    "        self.item_emb.weight.data.uniform_(0, 0.5)\n",
    "        \n",
    "        self.mse_calculator = torchmetrics.MeanSquaredError()\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        u = self.user_emb(u)\n",
    "        v = self.item_emb(v)\n",
    "        return (u*v).sum(1)  # taking the dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c06c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, train_df, epochs=10, lr=0.01, wd=0.0):\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        usernames = torch.LongTensor(train_df.userId.values)\n",
    "        game_titles = torch.LongTensor(train_df.movieId.values)\n",
    "        ratings = torch.FloatTensor(train_df.rating.values)\n",
    "        y_hat = model(usernames, game_titles)\n",
    "        loss = F.mse_loss(y_hat, ratings)\n",
    "        optimizer.zero_grad()  # reset gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e115036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_df):\n",
    "    model.eval()\n",
    "    usernames = torch.LongTensor(test_df.userId.values)\n",
    "    game_titles = torch.LongTensor(test_df.movieId.values)\n",
    "    ratings = torch.FloatTensor(test_df.rating.values)\n",
    "    y_hat = model(usernames, game_titles)\n",
    "    loss = F.mse_loss(y_hat, ratings)\n",
    "    print(\"test loss %.3f \" % loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:i3]",
   "language": "python",
   "name": "conda-env-i3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
